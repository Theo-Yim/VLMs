{
    "model_name_or_path": "OpenGVLab/InternVL3_5-8B",
    "use_flash_attn": true,
    
    "data_path": "/workspace/VLMs/utils/lh-poc/dataset/dataset_lh_theo_v2_train.json",
    "eval_data_path": "/workspace/VLMs/utils/lh-poc/dataset/dataset_lh_theo_v2_val.json",
    "image_folder": "/home/Theo-Yim/data/lh-poc/lh-data-image-train/",
    "image_size": 448,
    "max_dynamic_patches": 12,
    
    "lora_enable": true,
    "lora_r": 64,
    "lora_alpha": 128,
    "lora_dropout": 0.05,
    "lora_target_modules": [
        "q_proj",
        "k_proj", 
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "mlp1.1",
        "mlp1.3",
        "qkv",
        "fc1",
        "fc2"
    ],
    
    "output_dir": "/workspace/VLMs/InternVL3/checkpoints/internvl35_lora_lh_theo_v2_try2",
    "overwrite_output_dir": true,
    "do_train": true,
    "do_eval": true,
    
    "num_train_epochs": 4,
    "per_device_train_batch_size": 1,
    "gradient_accumulation_steps": 2,
    "eval_strategy": "steps",
    "eval_steps": 0.4,
    "save_strategy": "steps",
    "save_steps": 0.4,
    "save_total_limit": 5,
    "load_best_model_at_end": true,
    "metric_for_best_model": "eval_loss",
    
    "learning_rate": 1e-4,
    "max_grad_norm": 1.0,
    "weight_decay": 0.01,
    "warmup_steps": 100,
    "lr_scheduler_type": "cosine",
    "logging_steps": 10,
    
    "tf32": true,
    "fp16": false,
    "bf16": true,
    
    "dataloader_pin_memory": true,
    "dataloader_num_workers": 8,
    "dataloader_drop_last": true,
    "dataloader_persistent_workers": true,
    "remove_unused_columns": false,
    "gradient_checkpointing": false,
    
    "seed": 42,
    "report_to": "none",
    "logging_dir": "/workspace/VLMs/InternVL3/logs/internvl35_lora_lh_theo_v2"
}